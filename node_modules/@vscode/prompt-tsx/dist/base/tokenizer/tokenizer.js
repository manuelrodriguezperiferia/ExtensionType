"use strict";
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation and GitHub. All rights reserved.
 *--------------------------------------------------------------------------------------------*/
Object.defineProperty(exports, "__esModule", { value: true });
exports.Cl100KBaseTokenizer = void 0;
const tiktokenizer_1 = require("@microsoft/tiktokenizer");
const path_1 = require("path");
const openai_1 = require("../openai");
/**
 * The Cl100K BPE tokenizer for the `gpt-4`, `gpt-3.5-turbo`, and `text-embedding-ada-002` models.
 *
 * See https://github.com/microsoft/Tokenizer
 */
class Cl100KBaseTokenizer {
    constructor() {
        this.models = ['gpt-4', 'gpt-3.5-turbo', 'text-embedding-ada-002'];
        this.baseTokensPerMessage = openai_1.BaseTokensPerMessage;
        this.baseTokensPerName = openai_1.BaseTokensPerName;
        this.baseTokensPerCompletion = openai_1.BaseTokensPerCompletion;
    }
    /**
     * Tokenizes the given text using the Cl100K tokenizer.
     * @param text The text to tokenize.
     * @returns The tokenized text.
     */
    tokenize(text) {
        if (!this._cl100kTokenizer) {
            this._cl100kTokenizer = this.initTokenizer();
        }
        return this._cl100kTokenizer.encode(text);
    }
    /**
     * Calculates the token length of the given text.
     * @param text The text to calculate the token length for.
     * @returns The number of tokens in the text.
     */
    tokenLength(text) {
        if (!text) {
            return 0;
        }
        return this.tokenize(text).length;
    }
    /**
     * Counts tokens for a single chat message within a completion request.
     *
     * Follows https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb for GPT 3.5/4 models.
     *
     * **Note**: The result does not include base tokens for the completion itself.
     */
    countMessageTokens(message) {
        let numTokens = this.baseTokensPerMessage;
        for (const [key, value] of Object.entries(message)) {
            if (!value) {
                continue;
            }
            numTokens += this.tokenLength(value);
            if (key === 'name') {
                numTokens += this.baseTokensPerName;
            }
        }
        return numTokens;
    }
    /**
     * Counts tokens for a completion request.
     *
     * Follows https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb for GPT 3.5/4 models.
     */
    countCompletionTokens(messages) {
        let numTokens = 0;
        for (const message of messages) {
            numTokens += this.countMessageTokens(message);
        }
        numTokens += openai_1.BaseTokensPerCompletion;
        return numTokens;
    }
    initTokenizer() {
        return (0, tiktokenizer_1.createTokenizer)(
        // This file is copied to `dist` via the `build/postinstall.ts` script
        (0, path_1.join)(__dirname, './cl100k_base.tiktoken'), (0, tiktokenizer_1.getSpecialTokensByEncoder)('cl100k_base'), (0, tiktokenizer_1.getRegexByEncoder)('cl100k_base'), 64000);
    }
}
exports.Cl100KBaseTokenizer = Cl100KBaseTokenizer;
